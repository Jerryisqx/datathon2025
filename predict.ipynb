{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import csv\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_file(filepath):\n",
    "    \"\"\"Load and return a JSON file\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {filepath}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reject_id = []\n",
    "Accept_id = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get all the clients with consistent Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for list of valid clients...\n",
      "Found 772 valid clients\n",
      "Found 222 valid clients\n",
      "Found 772 valid client directories\n"
     ]
    }
   ],
   "source": [
    "def get_valid_clients(path):\n",
    "    \"\"\"Get list of valid clients from validation results file\"\"\"\n",
    "    valid_clients_file = os.path.join(os.getcwd(), path)\n",
    "    if not os.path.exists(valid_clients_file):\n",
    "        print(f\"Valid clients file not found: {valid_clients_file}\")\n",
    "        print(\"Please run account_valid.py first to generate this file.\")\n",
    "        return []\n",
    "    \n",
    "    with open(valid_clients_file, 'r') as f:\n",
    "        return [line.strip() for line in f.readlines()]\n",
    "\n",
    "root_dir = os.getcwd()\n",
    "# Get list of valid clients\n",
    "print(\"Looking for list of valid clients...\")\n",
    "valid_clients = get_valid_clients(\"validation_results_valid_clients.txt\")\n",
    "print(f\"Found {len(valid_clients)} valid clients\")\n",
    "invalid_clients = get_valid_clients(\"validation_results_invalid_clients.txt\")\n",
    "print(f\"Found {len(invalid_clients)} valid clients\")\n",
    "\n",
    "for invalid_id in invalid_clients:\n",
    "    Reject_id.append(f'{invalid_id}:Reject')\n",
    "\n",
    "client_paths = {}\n",
    "for folder in os.listdir(root_dir):\n",
    "    full_path = os.path.join(root_dir, folder)\n",
    "    if os.path.isdir(full_path) and folder.startswith(\"datathon_part\"):\n",
    "        for client_id in valid_clients:\n",
    "            client_path = os.path.join(full_path, client_id)\n",
    "            if os.path.exists(client_path):\n",
    "                client_paths[client_id] = client_path\n",
    "\n",
    "print(f\"Found {len(client_paths)} valid client directories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Filter again according to the valid passport number and birthdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_passport_data(passport_mrz: dict) -> dict:\n",
    "    line1, line2 = passport_mrz[0], passport_mrz[1]\n",
    "    line1_data = line1.split(\"<\")[:5]\n",
    "    middle_name = line1_data[-1].title()\n",
    "    country_code = line1_data[1][:3]\n",
    "    last_name = line1_data[1][3:].title()\n",
    "    first_name = line1_data[3].title()\n",
    "    line1_data = {\"first_name\": first_name, \"middle_name\": middle_name, \"last_name\": last_name, \"country_code\": country_code}\n",
    "\n",
    "    line2_data = line2.replace(\"<\", \"\")\n",
    "    passport_number = line2_data[:9]\n",
    "    country_code = line2_data[9:12]\n",
    "    birth_date = line2_data[12:][:2] + \"-\" + line2_data[12:][2:4] + \"-\" + line2_data[12:][4:]\n",
    "    if int(line2_data[12:][:2]) <= 10:\n",
    "        birth_date = \"20\" + birth_date\n",
    "    else:\n",
    "        birth_date = \"19\" + birth_date\n",
    "    line2_data = {\"passport_number\": passport_number, \"country_code\": country_code, \"birth_date\": birth_date}\n",
    "    return line1_data, line2_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client_971 does not have a valid passport\n",
      "client_371 does not have a valid passport\n",
      "client_47 does not have a valid passport\n",
      "client_117 does not have a valid passport\n",
      "client_914 does not have a valid passport\n",
      "client_180 does not have a valid passport\n",
      "client_544 does not have a valid passport\n",
      "client_776 does not have a valid passport\n",
      "client_915 does not have a valid passport\n",
      "client_747 does not have a valid passport\n",
      "client_400 does not have a valid passport\n",
      "client_294 does not have a valid passport\n",
      "client_453 does not have a valid passport\n",
      "client_250 does not have a valid passport\n",
      "client_611 does not have a valid passport\n",
      "client_618 does not have a valid passport\n",
      "client_875 does not have a valid passport\n",
      "client_617 does not have a valid passport\n",
      "client_550 does not have a valid passport\n",
      "client_738 does not have a valid passport\n",
      "client_362 does not have a valid passport\n",
      "client_570 does not have a valid passport\n",
      "client_742 does not have a valid passport\n",
      "client_986 does not have a valid passport\n",
      "client_74 does not have a valid passport\n",
      "client_16 does not have a valid passport\n",
      "client_653 does not have a valid passport\n",
      "client_3 does not have a valid passport\n",
      "client_4 does not have a valid passport\n",
      "client_458 does not have a valid passport\n",
      "client_247 does not have a valid passport\n",
      "client_155 does not have a valid passport\n",
      "client_351 does not have a valid passport\n",
      "client_506 does not have a valid passport\n"
     ]
    }
   ],
   "source": [
    "valid_pp_paths = {}\n",
    "for client_id, client_path in client_paths.items():\n",
    "    profile = load_json_file(os.path.join(client_path, \"client_profile.json\"))\n",
    "    passport = load_json_file(os.path.join(client_path, \"passport.json\"))\n",
    "    line1_data, line2_data = extract_passport_data(passport['passport_mrz'])\n",
    "    passport_number = line2_data['passport_number']\n",
    "    birth_date = line2_data['birth_date']\n",
    "    if passport_number != profile['passport_number'] or birth_date != profile['birth_date']:\n",
    "        print(f'{client_id} does not have a valid passport')\n",
    "        Reject_id.append(f'{client_id}:Reject')\n",
    "    else:\n",
    "        valid_pp_paths[client_id] = client_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "738"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_pp_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use LLM to check the description and profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Description to be ignored\n",
    "os.makedirs(\"tmp\", exist_ok=True)\n",
    "IGNORE_FIELDS = {\"Summary Note\", \"Client Summary\"}\n",
    "\n",
    "Des = []\n",
    "Pro = []\n",
    "labels = []\n",
    "id = 1\n",
    "chunk_num = 0\n",
    "for client_id, client_path in valid_pp_paths.items():\n",
    "    profile_path = os.path.join(client_path, \"client_profile.json\")\n",
    "    label_path = os.path.join(client_path, \"label.json\")\n",
    "    description_path = os.path.join(client_path, \"client_description.json\")\n",
    "\n",
    "    descriptions = load_json_file(description_path)\n",
    "    profile = load_json_file(profile_path)\n",
    "    label = load_json_file(label_path)\n",
    "    label = label['label']\n",
    "    labels.append(label)\n",
    "    for field in IGNORE_FIELDS:\n",
    "            descriptions.pop(field, None)\n",
    "\n",
    "    Des.append(descriptions)\n",
    "    Pro.append(profile)\n",
    "    if id % 30 == 0 or id == len(valid_pp_paths):#1000:\n",
    "        chunk_num += 1\n",
    "        with open(os.path.join(\"/Users/jerrychen/Desktop/datathon2025/tmp\", f\"embed_profile_{chunk_num}.json\"), \"w\", encoding=\"utf-8\") as f_out:\n",
    "            json.dump(Pro, f_out, indent=2, ensure_ascii=False)\n",
    "        with open(os.path.join(\"/Users/jerrychen/Desktop/datathon2025/tmp\", f\"embed_description_{chunk_num}.json\"), \"w\", encoding=\"utf-8\") as f_out:\n",
    "            json.dump(Des, f_out, indent=2, ensure_ascii=False)\n",
    "\n",
    "        Des = []\n",
    "        Pro = []\n",
    "\n",
    "    id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "  base_url=\"https://api.deepseek.com\",\n",
    "  api_key=\"sk-63f10246d168487cab095f760d437431\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 1 chunk\n",
      "Generating ...\n",
      "Generate Complete\n",
      "30\n",
      "processing 2 chunk\n",
      "Generating ...\n",
      "Generate Complete\n",
      "60\n",
      "processing 3 chunk\n",
      "Generating ...\n",
      "Generate Complete\n",
      "90\n",
      "processing 4 chunk\n",
      "Generating ...\n",
      "Generate Complete\n",
      "120\n",
      "processing 5 chunk\n",
      "Generating ...\n",
      "Generate Complete\n",
      "150\n",
      "processing 6 chunk\n",
      "Generating ...\n",
      "Generate Complete\n",
      "180\n",
      "processing 7 chunk\n",
      "Generating ...\n",
      "Generate Complete\n",
      "210\n",
      "processing 8 chunk\n",
      "Generating ...\n",
      "Generate Complete\n",
      "240\n",
      "processing 9 chunk\n",
      "Generating ...\n",
      "Generate Complete\n",
      "270\n",
      "processing 10 chunk\n",
      "Generating ...\n",
      "Generate Complete\n",
      "300\n",
      "processing 11 chunk\n",
      "Generating ...\n",
      "Generate Complete\n",
      "330\n",
      "processing 12 chunk\n",
      "Generating ...\n",
      "Generate Complete\n",
      "360\n",
      "processing 13 chunk\n",
      "Generating ...\n",
      "Generate Complete\n",
      "390\n",
      "processing 14 chunk\n",
      "Generating ...\n",
      "Generate Complete\n",
      "420\n",
      "processing 15 chunk\n",
      "Generating ...\n",
      "Generate Complete\n",
      "450\n",
      "processing 16 chunk\n",
      "Generating ...\n",
      "Generate Complete\n",
      "480\n",
      "processing 17 chunk\n",
      "Generating ...\n",
      "Generate Complete\n",
      "510\n",
      "processing 18 chunk\n",
      "Generating ...\n",
      "Generate Complete\n",
      "540\n",
      "processing 19 chunk\n",
      "Generating ...\n",
      "Generate Complete\n",
      "570\n",
      "processing 20 chunk\n",
      "Generating ...\n",
      "Generate Complete\n",
      "600\n",
      "processing 21 chunk\n",
      "Generating ...\n",
      "Generate Complete\n",
      "630\n",
      "processing 22 chunk\n",
      "Generating ...\n",
      "Generate Complete\n",
      "660\n",
      "processing 23 chunk\n",
      "Generating ...\n",
      "Generate Complete\n",
      "690\n",
      "processing 24 chunk\n",
      "Generating ...\n",
      "Generate Complete\n",
      "720\n",
      "processing 25 chunk\n",
      "Generating ...\n",
      "Generate Complete\n",
      "750\n"
     ]
    }
   ],
   "source": [
    "decisions = []\n",
    "text = []\n",
    "for idx in range(chunk_num):\n",
    "    print(f\"processing {idx+1} chunk\")\n",
    "    with open(f\"/Users/jerrychen/Desktop/datathon2025/tmp/embed_profile_{idx+1}.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        profile_template = f.read()\n",
    "\n",
    "    with open(f\"/Users/jerrychen/Desktop/datathon2025/tmp/embed_description_{idx+1}.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        client_description = f.read()\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    There are serveral clients information in two json files. For each client, keep strictly the form and structure of client_profile.json in mind as a template. Do as if you were filling in information based on this template. If there's information missing, pass to the next attribute in the client_profile.json, but if there's any information you extract from client_description - Copy.json to fill in the template different from the info in the original client_profile.json file (in the sense that the text info mismatches or any numerical value format mismatch or any numerical value mismatch in client_description - Copy.json for example it's extremely important that 1.00 and 1 are NOT considered as a match!) then print only Reject. Otherwise print OK.\n",
    "    For every client only given result once.\n",
    "    client_profile.json:\n",
    "    {profile_template}\n",
    "    client_description.json:\n",
    "    {client_description}\n",
    "    \"\"\"\n",
    "    print(\"Generating ...\")\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "    ],\n",
    "    stream=False\n",
    "    )\n",
    "    print(\"Generate Complete\")\n",
    "    # text.append(response.choices[0].message.content)\n",
    "    lines = response.choices[0].message.content.strip().split('\\n')\n",
    "    if idx != 25:\n",
    "        max_hits = 30\n",
    "    else:\n",
    "        max_hits = 18\n",
    "    hit = 0    \n",
    "    i = 0\n",
    "    while i < len(lines) and hit < max_hits:\n",
    "        line = lines[i]\n",
    "\n",
    "        if re.search(r'\\d', line):\n",
    "            j = i\n",
    "            while j < len(lines):\n",
    "                line_j = lines[j]\n",
    "                if \"OK\" in line_j:\n",
    "                    decisions.append(\"OK\")\n",
    "                    hit += 1\n",
    "                    break\n",
    "                elif \"Reject\" in line_j:\n",
    "                    decisions.append(\"Reject\")\n",
    "                    hit += 1\n",
    "                    break\n",
    "                j += 1\n",
    "        i += 1\n",
    "    print(len(decisions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok_idx = []\n",
    "# reject+idx = []\n",
    "for i in range(len(decisions)):\n",
    "    decision = decisions[i]\n",
    "    if decision == 'OK':\n",
    "        ok_idx.append(i)\n",
    "    if decision == 'Reject':\n",
    "        Reject_id.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "738"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_pp_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "ok_client_id = []\n",
    "for client_id, client_path in valid_pp_paths.items():\n",
    "    if i >= len(decisions):\n",
    "        break\n",
    "    decision = decisions[i]\n",
    "    if decision == 'OK':\n",
    "        ok_client_id.append(client_id)\n",
    "    if decision == 'Reject':\n",
    "        Reject_id.append(f'{client_id}:Reject')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. For rest clients, use random forest to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok_client_paths = {}\n",
    "for folder in os.listdir(root_dir):\n",
    "    full_path = os.path.join(root_dir, folder)\n",
    "    if os.path.isdir(full_path) and folder.startswith(\"datathon_evaluation\"):\n",
    "        for client_id in ok_client_id:\n",
    "            client_path = os.path.join(full_path, client_id)\n",
    "            if os.path.exists(client_path):\n",
    "                ok_client_paths[client_id] = client_path\n",
    "\n",
    "# print(f\"Found {len(ok_client_paths)} valid client directories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_profile(profile):\n",
    "    flat = {\n",
    "        'country_of_domicile': profile.get('country_of_domicile'),\n",
    "        'birth_date': profile.get('birth_date'),\n",
    "        'nationality': profile.get('nationality'),\n",
    "        'gender': profile.get('gender'),\n",
    "        'marital_status': profile.get('marital_status'),\n",
    "        'investment_risk_profile': profile.get('investment_risk_profile'),\n",
    "        'investment_horizon': profile.get('investment_horizon'),\n",
    "        'investment_experience': profile.get('investment_experience'),\n",
    "        'type_of_mandate': profile.get('type_of_mandate'),\n",
    "        'currency': profile.get('currency'),\n",
    "        'savings': profile.get('aum', {}).get('savings', 0),\n",
    "        'inheritance': profile.get('aum', {}).get('inheritance', 0),\n",
    "        'real_estate_value': profile.get('aum', {}).get('real_estate_value', 0),\n",
    "        'num_preferred_markets': len(profile.get('preferred_markets', [])),\n",
    "        'has_higher_education': int(bool(profile.get('higher_education'))),\n",
    "        'employment_history': profile.get(\"employment_history\", [])\n",
    "    }\n",
    "    # Calculate Age\n",
    "    from datetime import datetime\n",
    "    try:\n",
    "        birth = datetime.strptime(flat['birth_date'], '%Y-%m-%d')\n",
    "        flat['age'] = (datetime.today() - birth).days // 365\n",
    "    except:\n",
    "        flat['age'] = None\n",
    "\n",
    "    if flat[\"employment_history\"]:\n",
    "        last_job = flat[\"employment_history\"][-1]\n",
    "        retired = last_job.get(\"end_year\") is None\n",
    "    else:\n",
    "        retired = False\n",
    "    \n",
    "    flat['is_retired'] = retired\n",
    "\n",
    "    del flat['birth_date']\n",
    "    return flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok_profiles = []\n",
    "ok_labels = []\n",
    "ok_idx = []\n",
    "\n",
    "for client_id, client_path in ok_client_paths.items():\n",
    "    profile_path = os.path.join(client_path, \"client_profile.json\")\n",
    "    label_path = os.path.join(client_path, \"label.json\")\n",
    "\n",
    "    if not os.path.exists(profile_path):\n",
    "        print(f\"Warning: No client profile found for {client_id}\")\n",
    "        continue\n",
    "    # if not os.path.exists(label_path):\n",
    "    #     print(f\"Warning: No label file found for {client_id}\")\n",
    "    #     continue\n",
    "    profile = load_json_file(profile_path)\n",
    "    # label = load_json_file(label_path)\n",
    "\n",
    "    ok_profiles.append(flatten_profile(profile))\n",
    "    # ok_labels.append(label.get('label'))\n",
    "    ok_idx.append(client_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok_profile_pd = pd.DataFrame(ok_profiles)\n",
    "ok_label_pd = pd.Series(ok_labels, name='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ok_profile_pd\n",
    "# numeric_features = ['age', 'savings', 'inheritance', 'real_estate_value', 'num_preferred_markets']\n",
    "# categorical_features = [col for col in X.columns if col not in numeric_features]\n",
    "\n",
    "# preprocessor = ColumnTransformer([\n",
    "#     ('num', SimpleImputer(strategy='median'), numeric_features),\n",
    "#     ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "# ])\n",
    "ordinal_features = ['investment_risk_profile', 'investment_horizon', 'investment_experience']\n",
    "onehot_features = ['marital_status', 'type_of_mandate', 'has_higher_education', 'is_retired']\n",
    "numeric_features = ['savings', 'inheritance', 'real_estate_value', 'age']\n",
    "\n",
    "ordinal_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "])\n",
    "\n",
    "onehot_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('ord', ordinal_pipeline, ordinal_features),\n",
    "    ('ohe', onehot_pipeline, onehot_features),\n",
    "    ('num', numeric_pipeline, numeric_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = RandomForestClassifier(\n",
    "#     n_estimators=1000,          \n",
    "#     max_features='sqrt',      \n",
    "#     class_weight='balanced_subsample',   \n",
    "#     random_state=42)\n",
    "# model = make_pipeline(preprocessor, classifier)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "# model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"rf_model.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"rf_model.pkl\", \"rb\") as f:\n",
    "    loaded_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = loaded_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] == 'Accept':\n",
    "        Accept_id.append(f'{ok_idx[i]}:Accept')\n",
    "    elif y_pred[i] == 'Reject':\n",
    "        Reject_id.append(f'{ok_idx[i]}:Reject')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Decision = Accept_id + Reject_id\n",
    "sorted_data = sorted(Final_Decision, key=lambda x: int(x.split(':')[0].split('_')[1]))\n",
    "with open(\"solution_test.csv\", \"w\", newline='', encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f, delimiter=\";\")\n",
    "    for line in sorted_data:\n",
    "        writer.writerow(line.split(\";\"))  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
